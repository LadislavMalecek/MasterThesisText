\chapter{Proposed group recommender system}  \label{chap:our_work}

As we have discussed so far, group recommendation is not an easy task. There are many social and mathematical optimization criteria that are hard to define and even harder to measure. Let us now present our algorithm that aims to provide group recommendations that optimize fairness and relevance. We published this algorithm originally at UMAP'21 in \cite{our_ep_fuzz_da}.

\section{Preliminary}

In our context, we see fairness as a property that no user should be systematically discriminated against when it comes to their satisfaction with the recommended items. Optimizing this type of fairness is hard, especially when we have a user or a subset of the group whose preferences differ from the rest.

Some algorithms, such as the Average (AVG) and Least Misery(LM), already optimize for fairness (even if not primarily designed for that purpose). We can say that AVG strategy is fair due to its property that it considers all members' preferences as equally important. If the task is to recommend only a single item, then there is not much improvement to be made apart from how we define what we consider fair. However, that is different for tasks where we recommend a list of items or we have multiple following recommendation sessions. Then we can tweak our following recommendations to fix our previously introduced fairness imbalance.

Let us now classify approaches by how they optimize fairness into the following groups: \textit{item-wise}, \textit{list-wise}, and \textit{rank-sensitive} fairness.

Firstly, item-wise approaches optimize single-item performance. They usually apply some form of aggregation, such as average for the AVG or minimum for the LM approach. Scores for each item are calculated independently, and so scores for one item do not affect scores for any other.

Secondly, \textit{list-wise} approaches take into consideration which items have been recommended before. This allows us to balance fairness among group members over multiple consecutively recommended items. All before mentioned methods from Subsection \ref{subsec:specific_cases_of_fairness} fall into this category. In other words, items are not recommended independently, but the total fairness of items recommended in succession is important. The simplest example of a \textit{list-wise fairness} approach is FAI. We select for each user, in turn, their most preferred item. This way, at some point, each user will be satisfied, and FAI therefore indirectly balances the per-user utility.

We can define a per-user utility function $r_u; u \in G$, which measures the specified properties that we aim to optimize during the process of generating a list of recommended items. We can then use this utility function to control the recommendation of the following items to balance fairness among the group members. Most commonly, the utility function is a mean of predicted relevance for each user.

Moreover, lastly, \textit{rank-sensitive} approaches optimize fairness for each prefix of the recommended list of items. This approach has theoretically ideal properties in use cases where we do not know what portion of the recommended list will be consumed or viewed by the users. We only know about a single related approach from this category, GFAR \cite{GFAR-kaya2020}. As already described in Subsection \ref{subsec:03_advanced_methods.gfar}, the authors define fairness through the sum of probabilities that at least one item from the list will be relevant to each user.

Our proposed algorithm falls into the group of \textit{rank-sensitive} approaches. It maximizes the per-user proportional fraction of total relevance. Compared to GFAR, it does not define fairness based on the ranks of items. This gives it more freedom in the item selection process due to rank sensitive approach discounting the relevance of items with decreasing rank quite quickly, even if, in reality, items on lower ranks can still present a great option.

As already mentioned in our paper about this algorithm \cite{our_ep_fuzz_da}, the main inspiration for our work is election algorithms that are used for mandates allocation.
Mandate allocation is an important problem in distributing mandates based on the votes received. The important part of the problem is what to do if the votes are not divisible in the way that would allow to distribute them by a simple one mandate per a set number of votes without any remaining votes left unassigned.

We focus on D'Hondt's algorithm (DA). It, as already stated in Subsection \ref{subsec:03_advanced_methods.DHondtDO}, is commonly used in election systems around the world. It is a greedy selection algorithm that minimizes the number of votes that need to be left aside so that the remaining votes are represented exactly proportionally as described in \cite{wiki:dhondt_method}.

D'Hondt's algorithm procedure works in discrete steps, where in each step, it selects the party with the maximum quotient $quot$ one seat. The quotient for each party is calculated as the total number of votes the party has received $V_p$ divided by the number of seats $s_p$ + 1 that has already been awarded to that party $p$. Initially, $s_p$ is set to zero.

\begin{equation}
    quot(V_p, s_p) = \dfrac{V_p}{s_p + 1},
\end{equation}

Directly using the DA algorithm is not possible due to it being designed for discrete mandate allocation problems. But this unfortunate property has been addressed in \cite{fuzz_da}. We describe the method of D'Hondt's direct optimization in more detail in Subsection \ref{subsec:03_advanced_methods.DHondtDO}.

\section{EP-FuzzDA}

Let us now describe our algorithm of \textbf{E}xactly \textbf{P}roportional \textbf{F}uzzy \textbf{D} 'Hondt's \textbf{A}ggregation. It is a greedy algorithm that iteratively selects the best candidate item maximizing the marginal gains on \textbf{E}xactly-\textbf{P}roportional \textbf{rel}evance \textbf{sum} (EP-rel-sum) criterion.

As mentioned before, candidate allocation algorithm such as D'Hondt does not consider the varying relevance of candidate items nor the possible overlap of user preferences. Both these situations are very common in the domain of group recommender systems. We can probably find a set of items that closely balance the relevance between all group members. Nevertheless, each of these items may have a different candidate item that is a better fit for the group in the average rating but would lead to more imbalance. This could easily lead to recommending mediocre items only for the sake of balancing fairness.

We, therefore, want to maximize the balance of items' utility for each group member but without hindering the possible additional utility provided to members that overlap in preference with other members or are easier to satisfy. One solution is to optimize for the total item relevance but cap the maximum possible gain for each individual user so that it does not further add to the total item score. We call this cap \textit{exactly proportional relevance allowance}. It is calculated as a proportional share of each user on the total sum of all so far selected items' relevance.


Let us assume that we have already selected items $\mathcal{L}$ (this set can be empty), with individual relevance of $r_{i,u}$ for all items $i \in I$ and users $u$ of a group $\mathcal{G}$. We will use the example in Table \ref{table:6.2_relevance_example} for the following calculations.

\begin{table}[!ht]
    \centering
    \begin{tabular}{c | c c c | c c c c}
               &       &       &       &      &    & \multicolumn{2}{c}{EP-FuzzDA} \\
        Object & $u_1$ & $u_2$ & $u_3$ & AVG  & LM & step 1 & step 2\\
        \hline
        $c_1$ & 0.9 & 0.8 & 0.0 & 0.567 & 0.0 & 1.13 & - \\
        $c_2$ & 0.6 & 0.5 & 0.0 & 0.367 & 0.0 & 0.73 & 0.17 \\
        $c_3$ & 0.5 & 0.9 & 0.0 & 0.467 & 0.0 & 0.93 & 0.37 \\
        $c_4$ & 0.0 & 0.1 & 0.9 & 0.333 & 0.0 & 0.43 & 1.00 \\
        $c_5$ & 0.1 & 0.1 & 0.8 & 0.333 & 0.1 & 0.53 & 0.90 \\
        $c_6$ & 0.2 & 0.0 & 0.7 & 0.300 & 0.2 & 0.50  & 0.70 \\
    \end{tabular}
    \caption[Example of relevance calculation for item-wise aggregations]{Example of relevance calculation for item-wise aggregations. Least-misery, average, and two steps of our DA method. Step 2 is after selecting $c_1$ in step 1.}
    \label{table:6.2_relevance_example}
\end{table}


We then calculate the relevance allowance for a new candidate item as follows. First, we sum up relevance for all users for all already selected items as 
\begin{equation}
    TOT(\mathcal{G}, \mathcal{L}) = \sum_{i \in \mathcal{L}}\sum_{u\in \mathcal{G}} r_{i,u},
\end{equation}

Then we define $TOT_c$ for a candidate item $c$ (different from all already selected) as $TOT(\mathcal{G}, \mathcal{L} \cup c)$. This represents the prospected total utility if we would select the candidate $c$.




We then calculate the maximum allowed utility (relevance) with the appropriate weight of a user $w_u$ as:
\begin{equation}
    allowed\_utility_u = TOT_c * w_u.
\end{equation}

The weight of each group member can either be set to $1/|\mathcal{G}|$ or arbitrarily so that $\sum_{u \in \mathcal{G}} w_u = 1$. This weight represents how important each user is in the group and will scale the preferred utility ratio between group members if not set uniformly.

In our example in Table \ref{table:6.2_relevance_example}, at step 1. we have $TOT_{c1} = 0.9 + 0.8 + 0.0 = 1.7$ which scaled by uniform weight of $1/3$ to $ 1.7 * (1/3) = 0.57 = allowed\_utility_u$. We then calculate the unfulfilled relevance as the total received utility (from already selected items in $\mathcal{L}$) subtracted from the maximum allowed utility. We are selecting the first item. Therefore all users start with a received utility of 0, which makes the unfulfilled relevance for candidate item $c_1$ set to $0.57$ for all users.

\begin{equation}\label{eq:list_user_relevance}
    \mathit{total\_received\_utility}_u= \sum_{i \in \mathcal{L}} r_{u,i},
\end{equation}

\begin{equation}
    \mathit{unfulfilled\_rel}_u= max(0, allowed\_utility_u - \mathit{total\_received_utility}_u).
\end{equation}

We can then finally calculate the utility of each item by
\begin{equation}
    \mathit{rel}_i= \sum_{u \in \mathcal{G}} min(rel_{u,i}, \mathit{unfulfilled\_rel}_u).
\end{equation}


We can now calculate the utility of item $c1$ as $min(0.9, 0.57) + min(0.8, 0.57) + min(0.9, 0.57) = 1.13$. And similarly for all other candidate items. After the first round, we select $c1$ and follow into a second one. Now it starts to get interesting because we need to calculate the total received utility and use it to calculate the unfulfilled relevance correctly.

For item $c_2$ we have $TOT_{c_2} = 1.7 + 0.6 + 0.5 + 0.0 = 2.8$ (1.7 is $TOT$) and user $u_1$ we have unfulfilled relevance of $2.8 / 3 - 0.6 = 0.03$, for user $u_2$ we have $2.8 / 3 - 0.8 = 0.13$ and for $u_3$ we have $2.8 / 3 - 0.0 = 0.93$. All numbers are capped to the min 0. So in total the relevance of item $c_1$ is $min(0.6, 0.03) + min(0.5, 0.13) + min(0.93, 0.0) = 0.16$. We can see the output of relevance calculations for all items in the example table under column step 2. In the second step, we would therefore select item $c_4$.

Let us now be more formal with the definition of EP-rel-sum. We have a list of recommendations $\mathcal{L}$ and its total relevance $TOT$ as defined previously. Next, we have the $r_u$ total relevance of all items from $\mathcal{L}$ for user $u$ as defined with Equation \ref{eq:list_user_relevance}. Further, we have the per-user proportional relevance allowance $a_u$ ($\mathit{allowed\_utility_u}$)  as $TOT * w_u$. Weights $\sum_{u \in \mathcal{G}} w_u = 1$. 
Then we finally define the EP-rel-sum criterion as

\begin{equation}
    \textrm{EP-rel-sum}(\mathcal{L}_G) = \sum_{\forall u \in \mathcal{G}} \min(r_u, a_u).
\end{equation}

We can observe that for two lists $L_G^1$ and $L_G^2$ with the same total relevance, EP-rel-sum will be higher for the list more proportional to the weights $w_u$ distribution. Another way around, if we have two lists with the same relevance distribution, the one with higher total relevance will receive a higher EP-rel-sum score.

\begin{algorithm}\label{alg:fuzzy_dhondt}
    \caption{Exactly-Proportional Fuzzy D'Hondt's Aggregation}
    \begin{algorithmic}[1]
        \State {\bfseries Input:} group members $u \in \mathcal{G}$, candidate items $i \in \mathcal{I}$, relevance scores $r_{u,i} \in \mathbf{R}$, \#items to recommend $k$, user's weights $w_u$; $\sum w_u = 1$ 
        \State {\bfseries Output:} ordered list of group recommendations $\mathcal{L}_G$ of size $k$
        \vspace{1mm}
        \State $\mathcal{L}_G = []$ 
        \State $TOT = 0$
        \State $\forall u: r_u = 0$
        \vspace{1mm}
        \For{$1$ to $k$}
            \For{$i \in \mathcal{I} \setminus \mathcal{L}_G$}
                \State $TOT_c = TOT + \sum_{\forall u} r_{u,i}$
                \State $\forall u: \mathit{allowed\_utility}_u = TOT_i * w_u$
                \State $\forall u: \mathit{unfulfilled\_relevance}_u = max(0, \mathit{allowed\_utility}_u - r_u)$
                \State $gain_i = \sum_{\forall u} min(r_{u,i}, \mathit{unfulfilled\_relevance}_u)$
            \EndFor
            \State $i_{best} = \argmax_{\forall i}(gain_i)$
            \State append $c_{best}$ to $\mathcal{L}_G$
            \State  $\forall u: r_u = r_u + r_{u,i_{best}}$
            \State $TOT = \sum_{\forall u} r_u$
        \EndFor
        \State \textbf{return} $\mathcal{L}_G$ 
    \end{algorithmic}
\end{algorithm}

And finally, to make EP-rel-sum ranking sensitive, as previously described, we define marginal gains while extending the list of items as

\begin{equation}
    gain(\mathcal{L}_G, c_j) = \textrm{EP-rel-sum}(\mathcal{L}_G \cup \{c_j\}) - \textrm{EP-rel-sum}(\mathcal{L}_G).
\end{equation}

EP-FuzzDA, which we describe in Algorithm \ref{alg:fuzzy_dhondt}, iteratively selects the item that provides the maximal marginal gain to the already selected set of items. It, therefore, is a \textit{rank-sensitive} approach for optimizing fairness.

Another nice property not present in other group recommendation algorithms is that our algorithm naturally incorporates scaling of user preference importance using the weights $w_u$. We will use this later in our experiments for weighted groups and long-term fairness evaluation.
